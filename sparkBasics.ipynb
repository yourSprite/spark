{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T07:27:48.590177Z",
     "start_time": "2019-06-30T07:27:45.169396Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# 初始化SparkConf对象\n",
    "conf = SparkConf().setAppName('miniProject').setMaster('local[*]')\n",
    "sc = SparkContext.getOrCreate(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本地内存中已经有一份序列数据(比如python的list)，可以通过sc.parallelize去初始化一个RDD。当执行这个操作以后，list中的元素将被自动分块(partitioned)，并且把每一块送到集群上的不同机器上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T07:30:49.295734Z",
     "start_time": "2019-06-30T07:30:48.075961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[4] at parallelize at PythonRDD.scala:195\n",
      "8\n",
      "[[], [1], [], [2], [3], [], [4], [5]]\n"
     ]
    }
   ],
   "source": [
    "# 利用list创建一个RDD，parallelize()方法可以把list，numpy array，pdseries，datafram转成rdd\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "print(rdd)\n",
    "\n",
    "# getNumPartitions()查看list被分成了几部分\n",
    "print(rdd.getNumPartitions())\n",
    "\n",
    "# glom().collect()查看分区状况\n",
    "print(rdd.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建RDD的另一个方法是直接把文本读到RDD。文本的每一行都会被当做一个item，不过需要注意的一点是，Spark一般默认给定的路径是指向HDFS的，如果要从本地读取文件的话，给一个file://开头（windows下是以file:\\\\开头）的全局路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T07:41:56.482068Z",
     "start_time": "2019-06-30T07:41:56.183981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/wangyutian/code/python/spark\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'word count'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 记录当前pyspark工作环境\n",
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "\n",
    "# 要读入文件的全路径\n",
    "rdd = sc.textFile('file://'+cwd+'/wordcount.txt')\n",
    "\n",
    "# wholeTextFiles()读取真个文件夹的所有文件\n",
    "# rdd = sc.wholeTextFiles('file://'+cwd')\n",
    "\n",
    "# first()方法读取rdd数据第一个item\n",
    "rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T07:45:51.686583Z",
     "start_time": "2019-06-30T07:45:51.684115Z"
    }
   },
   "source": [
    "### rdd常用transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "map() 对RDD的每一个item都执行同一个操作  \n",
    "flatMap() 对RDD中的item执行同一个操作以后得到一个list，然后以平铺的方式把这些list里所有的结果组成新的list  \n",
    "filter() 筛选出来满足条件的item  \n",
    "distinct() 对RDD中的item去重  \n",
    "sample() 从RDD中的item中采样一部分出来，有放回或者无放回  \n",
    "sortBy() 对RDD中的item进行排序  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T07:49:48.399303Z",
     "start_time": "2019-06-30T07:49:48.151071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[17] at RDD at PythonRDD.scala:53\n",
      "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n",
      "[2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "numbersRDD = sc.parallelize(range(1, 10+1))\n",
    "print(numbersRDD)\n",
    "\n",
    "# map()对rdd的每一个item都执行同一个操作\n",
    "squaresRDD = numbersRDD.map(lambda x: x**2)\n",
    "print(squaresRDD.collect())\n",
    "\n",
    "# filter()筛选出来满足条件的item\n",
    "filterdRDD = numbersRDD.filter(lambda x: x % 2 == 0)\n",
    "print(filterdRDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T07:54:34.740448Z",
     "start_time": "2019-06-30T07:54:34.482254Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', 'My', 'name', 'is', 'yt']\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# flatMap() 对RDD中的item执行同一个操作以后得到一个list，然后以平铺的方式把这些list里所有的结果组成新的list\n",
    "sentencesRDD = sc.parallelize(['Hello world', 'My name is yt'])\n",
    "wordsRDD = sentencesRDD.flatMap(lambda sentence: sentence.split(' '))\n",
    "print(wordsRDD.collect())\n",
    "print(wordsRDD.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里如果使用map的结果是[[‘Hello’, ‘world’], [‘My’, ‘name’, ‘is’, ‘Patrick’]]，   \n",
    "使用flatmap的结果是全部展开[‘Hello’, ‘world’, ‘My’, ‘name’, ‘is’, ‘Patrick’]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T07:57:07.928871Z",
     "start_time": "2019-06-30T07:57:07.926272Z"
    }
   },
   "source": [
    "### 各个transformation可以串联使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T08:06:40.813383Z",
     "start_time": "2019-06-30T08:06:40.450368Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 10, 18, 14]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def doubleIfOdd(x):\n",
    "    if x % 2 == 1:\n",
    "        return 2 * x\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "numbersRDD = sc.parallelize(range(1, 10 + 1))\n",
    "resultRDD = numbersRDD.map(doubleIfOdd).filter(lambda x: x > 6).distinct()\n",
    "resultRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 当遇到更复杂的结构，比如被称作“pair RDDs”的以元组形式组织的k-v对（key, value），Spark中针对这种item结构的数据，定义了一些transform和action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduceByKey(): 对所有有着相同key的items执行reduce操作  \n",
    "groupByKey(): 返回类似(key, listOfValues)元组的RDD，后面的value List 是同一个key下面的  \n",
    "sortByKey(): 按照key排序  \n",
    "countByKey(): 按照key去对item个数进行统计  \n",
    "collectAsMap(): 和collect有些类似，但是返回的是k-v的字典  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T08:25:48.215264Z",
     "start_time": "2019-06-30T08:25:47.632139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', 3), ('york', 2), ('world', 1), ('new', 1), ('says', 1)]\n",
      "{'hello': 3, 'york': 2, 'world': 1, 'new': 1, 'says': 1}\n",
      "[('hello', 3), ('new', 1)]\n",
      "[('hello', 3), ('york', 2)]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(['Hello world', 'Hello New York', 'York says hello'])\n",
    "# 将word映射成(word, 1)并对所有有着相同key的items执行reduce操作\n",
    "resultRDD = rdd.flatMap(lambda sentence: sentence.split(' '))\\\n",
    "    .map(lambda word: word.lower())\\\n",
    "    .map(lambda word: (word, 1))\\\n",
    "    .reduceByKey(lambda x, y: x + y)\n",
    "print(resultRDD.collect())\n",
    "\n",
    "# collectAsMap类似collect,以k-v字典的形式返回\n",
    "result = resultRDD.collectAsMap()\n",
    "print(result)\n",
    "\n",
    "# sortByKey按键排序\n",
    "print(resultRDD.sortByKey(ascending=True).take(2))\n",
    "\n",
    "# 取出频次最高的两个词\n",
    "print(resultRDD.sortBy(lambda x: x[1], ascending=False).take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T08:22:05.700219Z",
     "start_time": "2019-06-30T08:22:05.626700Z"
    }
   },
   "source": [
    "## RDD间的操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T08:30:16.582123Z",
     "start_time": "2019-06-30T08:30:16.573603Z"
    }
   },
   "source": [
    "### 如果有2个RDD，可以通过下面这些操作，对它们进行集合运算得到1个新的RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rdd1.union(rdd2): 所有rdd1和rdd2中的item组合（并集）  \n",
    "rdd1.intersection(rdd2): rdd1 和 rdd2的交集  \n",
    "rdd1.substract(rdd2): 所有在rdd1中但不在rdd2中的item（差集）  \n",
    "rdd1.cartesian(rdd2): rdd1 和 rdd2中所有的元素笛卡尔乘积（正交和）  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T08:36:11.042090Z",
     "start_time": "2019-06-30T08:36:10.302999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3]\n",
      "[1]\n",
      "[(1, 2), (1, 3), (1, 4), (2, 2), (2, 3), (2, 4), (3, 2), (3, 3), (3, 4)]\n"
     ]
    }
   ],
   "source": [
    "# 初始化两个RDD\n",
    "numbersRDD = sc.parallelize([1, 2, 3])\n",
    "moreNumbersRDD = sc.parallelize([2, 3, 4])\n",
    "\n",
    "# intersection()取交集\n",
    "print(numbersRDD.intersection(moreNumbersRDD).collect())\n",
    "\n",
    "# substract()取差集\n",
    "print(numbersRDD.subtract(moreNumbersRDD).collect())\n",
    "\n",
    "# cartesian()取笛卡尔积\n",
    "print(numbersRDD.cartesian(moreNumbersRDD).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在给定2个RDD后，可以通过一个类似SQL的方式去join它们"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T08:40:33.191475Z",
     "start_time": "2019-06-30T08:40:32.473805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Brussels', ('John', 10)), ('Brussels', ('Jack', 10)), ('Antwerp', ('Jill', 7))]\n",
      "[('Brussels', ('John', 10)), ('Brussels', ('Jack', 10)), ('Antwerp', ('Jill', 7)), ('Leuven', ('Jane', None))]\n",
      "[('Brussels', ('John', 10)), ('Brussels', ('Jack', 10)), ('Antwerp', ('Jill', 7)), ('RestOfFlanders', (None, 5))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Brussels',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x10a40dbe0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x10a40d8d0>)),\n",
       " ('Antwerp',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x10a40da58>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x10a40d7b8>)),\n",
       " ('RestOfFlanders',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x10a40db70>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x10a40d898>)),\n",
       " ('Leuven',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x10a40d908>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x10a3dfb70>))]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Home of different people\n",
    "homesRDD = sc.parallelize([\n",
    "    ('Brussels', 'John'),\n",
    "    ('Brussels', 'Jack'),\n",
    "    ('Leuven', 'Jane'),\n",
    "    ('Antwerp', 'Jill'),\n",
    "])\n",
    "\n",
    "# Quality of life index for various cities\n",
    "lifeQualityRDD = sc.parallelize([\n",
    "    ('Brussels', 10),\n",
    "    ('Antwerp', 7),\n",
    "    ('RestOfFlanders', 5),\n",
    "])\n",
    "\n",
    "# join\n",
    "print(homesRDD.join(lifeQualityRDD).collect())\n",
    "# leftOuterJoin\n",
    "print(homesRDD.leftOuterJoin(lifeQualityRDD).collect())\n",
    "# rightOuterJoin\n",
    "print(homesRDD.rightOuterJoin(lifeQualityRDD).collect())\n",
    "# cogroup\n",
    "homesRDD.cogroup(lifeQualityRDD).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T08:41:41.405098Z",
     "start_time": "2019-06-30T08:41:41.147484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Brussels', (['John', 'Jack'], [10])),\n",
       " ('Antwerp', (['Jill'], [7])),\n",
       " ('RestOfFlanders', ([], [5])),\n",
       " ('Leuven', (['Jane'], []))]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Oops!  Those <ResultIterable>s are Spark's way of returning a list\n",
    "# that we can walk over, without materializing the list.\n",
    "# Let's materialize the lists to make the above more readable:\n",
    "(homesRDD\n",
    " .cogroup(lifeQualityRDD)\n",
    " .map(lambda x:(x[0], (list(x[1][0]), list(x[1][1]))))\n",
    " .collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T08:42:04.867696Z",
     "start_time": "2019-06-30T08:42:04.865080Z"
    }
   },
   "source": [
    "## 惰性计算，actions方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**特别注意：Spark的一个核心概念是惰性计算。当你把一个RDD转换成另一个的时候，这个转换不会立即生效执行！！！Spark会把它先记在心里，等到真的有actions需要取转换结果时，才会重新组织transformations(因为可能有一连串的变换)。这样可以避免不必要的中间结果存储和通信。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T08:43:31.686034Z",
     "start_time": "2019-06-30T08:43:31.683485Z"
    }
   },
   "source": [
    "### 常见的action如下，当它们出现的时候，表明需要执行上面定义过的transform了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T08:43:56.376296Z",
     "start_time": "2019-06-30T08:43:56.370094Z"
    }
   },
   "source": [
    "collect(): 计算所有的items并返回所有的结果到driver端，接着 collect()会以Python list的形式返回结果  \n",
    "first(): 和上面是类似的，不过只返回第1个item  \n",
    "take(n): 类似，但是返回n个item  \n",
    "count(): 计算RDD中item的个数  \n",
    "top(n): 返回头n个items，按照自然结果排序  \n",
    "reduce(): 对RDD中的items做聚合  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T08:45:30.079140Z",
     "start_time": "2019-06-30T08:45:29.979042Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(1, 10+1))\n",
    "rdd.reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 有时候需要重复用到某个transform序列得到的RDD结果。但是一遍遍重复计算显然是要开销的，所以我们可以通过一个叫做cache()的操作把它暂时地存储在内存中。缓存RDD结果对于重复迭代的操作非常有用，比如很多机器学习的算法，训练过程需要重复迭代。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T08:49:00.657157Z",
     "start_time": "2019-06-30T08:49:00.364625Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "numbersRDD = sc.parallelize(np.linspace(1.0, 10.0, 10))\n",
    "squaresRDD = numbersRDD.map(lambda x: x**2)\n",
    "\n",
    "squaresRDD.cache()\n",
    "\n",
    "avg = squaresRDD.reduce(lambda x, y: x + y)/squaresRDD.count()\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
